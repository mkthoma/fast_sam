# Fast SAM

In April 2023, Meta Research took the computer vision world by storm with the release of the Segment Anything Model (SAM), a powerful image segmentation model. This model has strong segmentation capabilities: given a photo, SAM can, with a high degree of precision, generate masks that segment objects in the image. SAM was trained on the SA-1B dataset of over 11 billion segmentation masks.

FastSAM, released by the Image and Video Analysis Group at the Chinese Academy of Sciences, is an image segmentation model trained on 2% of the data used to train the groundbreaking Segment Anything Model (SAM). FastSAM has lesser computational requirements than SAM, while still achieving strong accuracy.

FastSAM was trained using the Ultralytics YOLOv8 instance segmentation architecture. This is interesting because it shows the strength of the dataset on which the original SAM was trained: using only a portion of the dataset, researchers were able to create a model that segments objects in images with relatively precise boundaries.

In addition, the FastSAM authors report a 50x faster run-time speed, making the model more practical to run than the original SAM model.

![image](https://github.com/mkthoma/fast_sam/assets/135134412/d78cb405-0c89-4a66-852b-b964c9d07e86)


FastSAM works using a two-stage process:
1. A pre-trained YOLOv8 instance segmentation model generates segmentation masks, then;
2. Optionally, a text prompt can be specified to return masks related to the prompt. This is done using CLIP. CLIP is run over each mask and images with a close similarity to the text prompt are returned as a match.

The FastSAM authors note that the model can also be used for zero-shot edge detection and object proposal generation.

In the case of zero-shot edge detection, the FastSAM authors reported that, through qualitative observation, "despite FastSAM’s significantly fewer parameters (only 68M), it produces a generally good edge map.” The following image, from the FastSAM paper, shows the quality of FastSAM edge boundaries:

![image](https://github.com/mkthoma/fast_sam/assets/135134412/b4983e24-bdd0-4706-a8c4-1888f1e75702)

In this example, we can see the boundaries are less precise than those of SAM, but are nonetheless impressive given the model is trained on a significantly smaller dataset than SAM.

FastSAM is an image segmentation model trained on a portion of the dataset on which Meta Research’s SAM model was trained. Inference on FastSAM, as the name suggests, is faster than that of the SAM model. Fast Segment Anything could be used as a transfer-learning checkpoint, and demonstrates the quality of the SAM dataset. With that said, masks from FastSAM are less precise than masks generated by SAM.

A sample live implementation can be found [here](https://huggingface.co/spaces/mkthoma/fastSAM).

## Examples

#### Segmenting a dog:

![image](https://github.com/mkthoma/fast_sam/assets/135134412/88f7a5a2-de87-4f86-b7c2-b582f6cecd98)

#### Segmenting a cat:

![image](https://github.com/mkthoma/fast_sam/assets/135134412/ee2058ec-964f-4d26-b4e4-ba8314f22341)

#### Segmenting the sky:

![image](https://github.com/mkthoma/fast_sam/assets/135134412/3b2d87c9-0baf-451b-8fb9-16bc2a9ac678)
